# 调研报告——校园规模的分布式文件共享系统



## 一、项目背景

- ### 时代背景

- ### 分布式文件系统概念

- ### 对共享经济的思考

- ### 在校大学生对共享文件系统的需求

  ​

## 二、立项依据

- ###传统解决方案的局限性

- ###校园规模的文件共享系统需要具备的特性

- ###框架设计及技术依据

  ​

## 三、前瞻性/重要性分析



## 四、相关工作

- ###成熟的分布式文件系统
- ###校园规模的分布式文件系统案例















## 一、项目背景

### 1. 时代背景

   - ###随着数据中心数据信息的增加和互联网应用的多样化，大量半结构化和非结构化数据涌现，大数据时代来临。在大数据分析和处理技术吸引了越来 越多关注的同时，作为其底层支持的海量数据存储系统成为研究的热点。 大数据时代的海量数据存储呈现出新的特点：1）数据规模不断扩大，文件 数量急剧增长，一些大型的互联网公司如Google，腾讯等的数据规模已突 破PB量级，需要管理的文件数达到亿级；2）访问并发度高。互联网信息服 务通常面对大量的用户，同时在线人数也达数千万。大量的用户并发访问 造成大量的随机读写，对存储系统的元数据性能和文件访问延迟带来很大 的挑战；3）数据结构和处理需求呈现多样化，包括离线数据分析类应用和 在线并发访问类应用，经常需要24h不间断服务，对系统的可靠性要求越来 越高。
   - ###在这种背景下，人们对分布式文件系统的研究也越来越深入，目前为止已 经出现了一批以Linux为基础的分布式文件系统，以支持高可靠、高性能、 高并发的应用需求。

   ​

### 2. 分布式文件系统概念

- ###相对于本机端的文件系统而言,分布式文件系统(Distributed File System， DFS)是一种允许文件通过网络在多台主机上分享的文件系统。在这样的文 件系统中,客户端并非直接访问底层的数据存储区块,而是通过网络,以特定 的通信协议和服务器沟通。借由通信协议的设计,可以让客户端和服务器端 都能根据访问控制清单或是授权,来限制对于文件系统的访问。相对地,在一 个分享的磁盘文件系统中,所有节点对数据存储区块都有相同的访问权,在这 样的系统中,访问权限就必须由客户端程序来控制。

   ​

### 3. 对共享经济的思考

- ###近年来共享经济发展迅速，共享单车、共享租车、共享充电宝、共享雨伞等莫不如是。诚然，不同用户在实物资源方面的闲置与缺失的矛盾是最突出的，但随着信息互联网基础设施的完善以及人群生活中所接触的必要资源种类日益增多，技术、资源等非实物方面的共享需求将变得更大。例如，很多人的电脑硬盘存储空间大多数时候用不完，过剩也造就了资源的浪费，同时，与之相关的企业数据中心等场所又面临巨大的存储压力。为何不把过剩的存储空间共享出来？这样不仅资源的利用率将会大大增加，同时存储成本也能大大降低。

   ​

### 4. 在校大学生对共享文件系统的需求

- ###在一所大学中，校园中的同学需要频繁的从网络上下载各种文件。这些文件中可能包含音频文件、影音文件，游戏文件；也有可能诸如电子图书、教学课件的文档文件。由于同一校园内同学们所需要的从网络上下载的资源有很大的相同，因此同学们分别到网上去搜索并下载自己需要的资源势必不如从一个校园内的文件系统上下载文件有效率。基于以上几点事实和考量，我们认为可以在校园内搭建一个校园规模的文件共享系统。

   ​

## 二、立项依据

### 1. 传统解决方案的局限性

- ###学校官方搭建资源网站

  ###可以搭建一个校园官方的资源站，同学们都访问该网站来下载自己所需要的资源。这种解决方案的优点是架构简单，方便搭建；但存在着以下缺点：

  - ###开销大

    ###这种方案需要一个中心化的服务器来存储的数据，而海量的数据需要大量的存储服务器来存储数据。这对学校而言将是一笔不小的开销。      

  - ###扩展性差

    ###![1524401730248](C:\Users\hangy\Desktop\FastDFS\1524401730248.png)

    ###如果当前的存储容量不够时，需要购入新的服务器来存储数据，可扩展性差

  - ###隐私性差

    ###由于是中心化的文件系统，必然有学校官方的机构来对该系统进行管理和维护，这就意味着该文件系统中的资源完全由学校官方进行监控，隐私性差

  - ###对网络带宽要求高

    ![1523799046360](C:\Users\hangy\Desktop\FastDFS\IPFS调研\picture\1523799046360.png)

    ​

    ###由于是中心化的文件系统，当大量用户同时从这个文件系统下载资源时，网络带宽的压力很大，从而导致下载速度很慢

  - ###存在单点故障问题

- ###网盘

  - ###私密性差

    ###由于存储在网盘上的资源都被网盘服务提供商所控制，服务商可能查看并监管资源，私密性较差。

  - ###传输速度慢

    ###用户上传下载数据时严重依赖当时带宽以及运营商所开放的带宽，速度波动较大。当运营商限速时，哪怕用户的网络再流畅，用户的使用体验也极差。

    ​


### 2. 校园规模的文件共享系统需要具备的特性

- ###去中心化

- ###传输速度快

- ###可扩展性高

- ###容错性高

  ​

### 3. 框架设计及依据

- ###文件存储：每个用户贡献出一定的磁盘存储空间，将文件分块后分散存储到各个用户的硬盘上

  - ###纠删码(Erasure Code)对文件进行分块

    ![1524142931615](C:\Users\hangy\Desktop\FastDFS\1524142931615.png)

    ###Erasure Code（EC），即纠删码，是一种前向错误纠正技术（Forward Error Correction，FEC），主要应用在网络传输中避免包的丢失，存储系统利用它来提高存储可靠性。相比多副本复制而言，纠删码能够以更小的数据冗余度获得更高数据可靠性，但编码方式较复杂，需要大量计算。纠删码只能容忍数据丢失，无法容忍数据篡改，纠删码正是得名与此。

    ###Erasure Coding的主要原理就是将待存储的文件首先分成n块，之后利用算法计算出维持存储冗余所需要的额外m校验块；最后将数据块和校验快分散存储到各个存储服务器上

    ###可以选择存储文件被分割成的存储块的数目以及校验块的数目，从而获得不同的容错性能

    ###当某个存储服务器发生错误时，Erasure Coding可以根据校验块以及当前可用的存储块中的任意n块重建数据

    ###在传统的分布式文件系统中，文件备份大多数都是选择三备份的策略。但是这对于存储空间的利用率不高。与之相比，EC码在相对高的存储利用率（大概有1：1.5）上，实现了相对不错的数据可靠性。

  - ###DHT将文件分布存储

    ![1523532562853](C:\Users\hangy\Desktop\FastDFS\可行性报告\picture\1523532562853.png)

    ​

    - ###DHT全称叫分布式哈希表(Distributed Hash Table)，是一种分布式存 储方法。它提供的服务类似于hash表，键值对存储在DHT中，任何 参与该结构的节点能高效的由键查询到数据值。这种键值对的匹配 是分布式的分配到各个节点的，这样节点的增加与删减只会带来比 较小的系统扰动。这样也可以有效地避免“中央集权式”的服务器（比 如：tracker）的单一故障而带来的整个网络瘫痪。 

    - ###DHT技术本质上强调以下特性： 

       - ###离散性：构成系统的节点并没有任何中央式的协调机制。 
       - ###伸缩性：即使有成千上万个节点，系统仍然应该十分有效率。 
       - ###容错性：即使节点不断地加入、离开或是停止工作，系统仍然 必须达到一定的可靠度。

    - ###其关键技术为：任一个节点只需要与系统中的部分节点（通常为 O(logN)个）沟通，当成员改变的时候，只有一部分的工作（例如数 据或键的发送，哈希表的改变等）必须要完成。基本上，DHT技术就 是一种映射key和节点的算法以及路由的算法。 

    - ###DHT的结构：关键值空间分区(keyspace partitioning)和延展网络 (overlay network)  

       ###关键值空间分区是指每一个节点掌管部分键空间。  

       ###延展网络是指一个连接各个节点的抽象网络，它能使每个节点找到拥有特定键的节点。每个节点或者存储了该键，或者存储离这个键更近的节点连接 

    - ###当这些组件都准备好后,一般使用分布式散列表来存储与读取的方式 如下所述。假设关键值空间是一个 160 位长的字符串集合。为了在 分布式散列表中存储一个文件,名称为filename 且内容 data,我们计算 filename 的 SHA1 散列值（一个 160 位的关键值k）并将消息 put(k,data)送给分布式散列表中的任意参与节点。此消息在延展网络 中被转送,直到抵达在关键值空间分区中被指定负责存储关键值 k 的 节点。而 (k,data)即存储在该节点。其他的节点只需要重新计算 filename 的散列值 k,然后提交消息 get(k)给分布式散列表中的任意参 与节点,以此来找与 k 相关的数据。此消息也会在延展网络中被转送 到负责存储 k 的节点, 而此节点则会负责传回存储的数据 data。

       ​

    ​

- ###文件定位

  - ###DHT路由算法

    ###当某个节点接收到查询数据的请求，也即接收到一个key值后，它首先根据自己的ID以及这个key值计算自己与该数据之间的距离，然后在计算它所知道的其他节点与这个key的距离。

    ###若计算结果表明该结点据该数据最近，该结点就在自己的存储空间中寻找该key值所对应的数据。如果没有找到则报错。

    ###若计算结果表明另一个节点N'据该数据最近，则该结点就把这个key值发送给节点N'。N‘在接收到key值之后，重复上述的过程，直至找到文件。

- ###文件下载

  - ###Bit Torrent

    ![1524140850355](C:\Users\hangy\Desktop\FastDFS\1524140850355.png)

    ​

    - ###根据BitTorrent协议，文件发布者会根据要发布的文件生成提供一个.torrent文件，即种子文件，也简称为“种子”。

    - ###种子文件本质上是文本文件，包含Tracker信息和文件信息两部分。Tracker信息主要是BT下载中需要用到的Tracker服务器的地址和针对Tracker服务器的设置，文件信息是根据对目标文件的计算生成的，计算结果根据BitTorrent协议内的Bencode规则进行编码。它的主要原理是需要把提供下载的文件虚拟分成大小相等的块，块大小必须为2k的整数次方（由于是虚拟分块，硬盘上并不产生各个块文件），并把每个块的索引信息和Hash验证码写入种子文件中；所以，种子文件就是被下载文件的“索引”。

    - ###下载者要下载文件内容，需要先得到相应的种子文件，然后使用BT客户端软件进行下载。

    - ###下载时，BT客户端首先解析种子文件得到Tracker地址，然后连接Tracker服务器。Tracker服务器回应下载者的请求，提供下载者其他下载者（包括发布者）的IP。下载者再连接其他下载者，根据种子文件，两者分别告知对方自己已经有的块，然后交换对方所没有的数据。此时不需要其他服务器参与，分散了单个线路上的数据流量，因此减轻了服务器负担。

    - ###下载者每得到一个块，需要算出下载块的Hash验证码与种子文件中的对比，如果一样则说明块正确，不一样则需要重新下载这个块。这种规定是为了解决下载内容准确性的问题

    - ###一般的HTTP/FTP下载，发布文件仅在某个或某几个服务器，下载的人太多，服务器的带宽很易不胜负荷，变得很慢。而BitTorrent协议下载的特点是，下载的人越多，提供的带宽也越多，下载速度就越快。同时，拥有完整文件的用户也会越来越多，使文件的“寿命”不断延长。

    ​

## 三、前瞻性/重要性

- ###校园规模的分布式文件共享系统可以满足同学对于资源共享及存储的平台的需求，同时由于其去中心化的设计，一定程度上减少了学校相关机构对同学之间共享的数据被进行监管和控制。
- ###这个文件系统特点在于系统中的每个用户既是文件的贡献者也是文件的享有者同时也是文件的存储者，这样的特点使得整个系统具有较好的可扩展性
- ###目前传统的中心化文件存储需要投入专用的存储设备，设备的投入和更换代价都很大，并且可扩展性差，可能造成单点故障。可见去中心化的分布式文件系统的重要性
- ###在基于分布式文件系统的NAS集群上应用共享的概念，让用户贡献存储空间而不是搭建一些集中服务器提供存储空间，让存储的成本更低，效率更高，与未来互联网的发展趋势相符，无疑会有广阔的发展前景。



## 四、相关工作

1. ###成熟的分布式文件系统

   - ###Ceph

     - ###Ceph分布数据的过程：首先计算数据x的Hash值并将结果和PG（分 区）数目取余，以得到数据x对应的PG编号。然后，通过CRUSH算法 将PG映射到一组OSD中。最后把数据x存放到PG对应的OSD中。这个 过程中包含了两次映射，第一次是数据x到PG的映射。如果把PG当作 存储节点，那么这和文章开头提到的普通Hash算法一样。不同的 是，PG是抽象的存储节点，它不会随着物理节点的加入或则离开而 增加或减少，因此数据到PG的映射是稳定的
     - ###Ceph文件系统拥有优秀的性能、高可靠性和高可扩展性这三大特 性。优秀的性能是指数据能够在各个节点上进行均衡地分布；高可靠性表示在Ceph文件系统中没有单点故障，并且存储在系统中的数 据能够保证尽可能的不丢失；高可扩展性即Ceph系统易于扩展，能 够很容易实现从TB到PB级别的扩展。

   - ###glusterfs

     - ###GlusterFS采用独特的无中心对称式架构，与其他有中心的分布式文 件系统相比，它没有专用的元数据服务集群。在文件定位的问题 上，GlusterFS使用DHT算法进行文件定位，集群中的任何服务器和 客户端只需根据路径和文件名就可以对数据进行定位和读写访问。 换句话说，GlusterFS不需要将元数据与数据进行分离，因为文件定 位可独立并行化进行。

     - ###数据访问流程：

       ###使用Davies-Meyer算法计算32位hash值，输入参数为文件名。根据hash值在集群中选择子卷（存储服务器），进行文件定位。对所选择的子卷进行数据访问。

   - ###Lustre

     - ###Lustre分布式文件系统是1999年卡内基梅隆大学的Peter J.Braam所 发起的研究项目，Lustre是典型的元数据中心式的分布式文件系统， MDS（元数据服务器集群）用于存储文件的元数据信息，OSS（对 象存储服务器）用于存储实际的文件数据。 

     - ###Luster支持文件整体存储和文件分片两种存储方式。整体存储就是将 文件整体存储在OST（对象存储目的地）中；而分片存储就是将文件 分块存储在子目录Stripe下，Stripe有size（文件分片大小）， ost（起始存储位置），和count（分片数量）三个参数。

     - ###Lustre的优点是可以设置具体子目录Stripe的参数，从而灵活的存储 不同大小的文件，相比于单一的整体存储方式更为高效。但是Lustre 不提供数据保护，也存在着元数据存储上单点问题，这两点决定了 Lustre在安全性和容错性上的短板

   - ### GFS( Google File System )

     - ### GFS是Google公司为了存储海量搜索数据而设计的专用文件系统，其 最为鲜明的特点是独特的容错机制以及自动负载均衡技术。GFS系统 中包括一个独立的GFS Master控制服务器，多台GFS Chunkserver数 据块服务器以及多个GFS Client客户端。

     - ### 和HDFS一样，上传到GFS中的文件都被分割成固定大小的Chunk，每 一个Chunk在不同的Chunkserver中保存多份。Chunk数据块用以存储 Chunk数据，并根据指定的Chunk标识符和字节范围来读写块数据。 

     - ### GFS Master存储Chunk的元数据，接受来自各个Chunkserver的周期 性信息，并向各个Chunkserver发送指令

     - ### 在GFS中，GFS服务器生成诊断日志用于记录许多关键事件（如 Chunkserver的启动和关闭）以及所有的远程调用的请求和相应。在 空间允许的情况下，GFS会尽量保存这些日志。在系统发生故障后， 通过对远程调用日志的分析，可以重建交互历史从而诊断出错误

       ​

2. ###校园规模的分布式文件系统案例

   - ###CorsairFS是一种针对校园网和企业内部网设计的专用分布式文件系统通过采用可扩展的架构、分块存储方式和基于注册、汇报的自组织机制，系统在满足性能和吞吐量指标的前提下，具有更好的可扩展性和可管理性，能够提供同一组数据的多种不同视图，允许在不要动数据的情况下对目录结构进行重构

   - ###系统针对大量用户的数据共享和存储的工作负载进行了优化，能有效处理大量小文件的并发访问

   - ###Corsair FS的存储服务器集群仍然是由学校相关人员进行维护，并不是一个去中心化的系统

     ​


## *参考文献

###[1] 刘立坤, 武永卫, 徐鹏志,等. CorsairFS:一种面向校园网的分布式文件系统[J]. 西安交通大学学报, 2009, 43(8):43-47.

###[2] Stoica I, Morris R, Karger D, et al. Chord: A scalable peer-to-peer lookup service for internet applications[C]// Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications. ACM, 2001:149-160.

###[3] Lakshman A, Malik P. Cassandra:a decentralized structured storage system[J]. Acm Sigops Operating Systems Review, 2010, 44(2):35-40.

###[4] 刘胜利 去 中 心 化 的 安 全 分 布 式 文 件 系 统 上海交通大学计算机科学与工程系 2012

###[5] M. Tim Jones (2010-06-04). "Ceph: A Linux petabyte-scale distributed file system" (PDF). IBM. Retrieved 2014-12-03.

###[6] "BlueStore". Ceph. Retrieved 2017-09-29.

###[7] R. Srikant and Dongyu Qiu. Modeling and performance analysis of BitTorrent-like peer-to-peer networks

###[8] Kaashoek M F, Karger D R. Koorde: A Simple Degree-Optimal Distributed Hash Table[J].  2003, 2735:98-107.

###[9]Zhang H, Wen Y, Xie H, et al. Distributed Hash Table[M]. Springer New York, 2013.

###[10] Boyer E B, Broomfield M C, Perrotti T A. GlusterFS One Storage Server to Rule Them All[J].  2012.

###[11] Donovan S, Symposium L, Kleen A, et al. Lustre: Building a File System for 1,000-node Clusters[J]. Proceedings of the Linux Symposium, 2003:9.



